---
title: <span style="color:#686868"> **Residuales**</span>
author: "Métodos y Simulación Estadística"
output:
  html_document:
    code_folding: hide
    css: style.css
---

</br></br>
<h2>Introducción</h2>


Los **residuales** o **residuos** en regresión son herramientas fundamentales para evaluar si los supuestos  del modelo se cumplen y detectar posibles problemas en los datos. Existen varios tipos de residuales, cada uno con un propósito específico:

- El **residual ordinario** es la diferencia directa entre los valores observados y los predichos por el modelo.

- Los **residuales estandarizados y studentizados** ayudan a identificar **valores atípicos**, ajustando la escala de los residuales para hacerlos comparables.

- El **residual PRESS** es útil en **validación cruzada**, ya que evalúa el impacto de cada observación en la predicción al excluirla del ajuste.



</br></br>
<h2>Tipos de residuales en modelos de regresión</h2>


Para comprender mejor estos residuales, es importante conocer algunos conceptos clave:

- **Peso \( W_i \)**: Indica la importancia de cada observación en el modelo. En modelos sin ponderación, se asume \( W_i = 1 \).

- **Leverage \( h_{ii} \)**: Representa la influencia de la observación \( i \) en su propia predicción. Se obtiene a partir de la **matriz sombrero** o **hat matrix** que se determina con la matriz de diseño $X$:

  \[
  H = X (X^\top X)^{-1} X^\top.
  \]

  Valores altos de \( h_{ii} \) sugieren que una observación tiene un impacto significativo en la estimación de los coeficientes y en la predicción.

- **Varianza ajustada \( \hat{\sigma}^2_{(i)} \)**: Es la estimación de la varianza de los errores cuando **se excluye** la observación \( i \). Se utiliza para evaluar la estabilidad del modelo ante la eliminación de datos individuales.

- **Estimación \( \hat{y}_{(i)} \)**: Representa la predicción de \( y_i \) en un modelo ajustado **sin incluir la observación \( i \)**. Su cálculo es clave en métodos de validación cruzada, como el residual PRESS.


El análisis de los distintos tipos de residuales permite detectar **valores atípicos, puntos influyentes y problemas de ajuste** en el modelo. Comprender cómo estos residuales se calculan e interpretan es fundamental para garantizar la validez del modelo de regresión y mejorar su capacidad predictiva.

---

</br></br>
<h3>Residual ordinario</h3>

El **residual ordinario** mide la diferencia entre el valor observado \( y_i \) y el valor predicho \( \hat{y}_i \), es decir, representa el error de predicción para cada observación en el modelo de regresión.

\[
e_i = y_i - \hat{y}_i
\]

Donde:

- \( e_i \) es el **residual ordinario** para la observación \( i \).

- \( y_i \) es el **valor observado** de la variable respuesta.

- \( \hat{y}_i \) es el **valor ajustado (predicho)** por el modelo de regresión.

Los residuales ordinarios permiten evaluar la **precisión del modelo**, ya que indican qué tan lejos están las predicciones de los valores reales. Cuando los residuos presentan un comportamiento aleatorio, sugiere que el modelo es adecuado; sin embargo, **patrones sistemáticos en los residuales pueden indicar problemas en la especificación del modelo**.




</br></br>
<h3>Residual de Pearson</h3>

El **residual de Pearson** es una versión ponderada del **residual ordinario**, que ajusta la diferencia entre los valores observados y predichos utilizando un factor de ponderación \( W_i \):

\[
p_i = e_i \sqrt{W_i}
\]

donde:

- \( p_i \) es el **residual de Pearson** para la observación \( i \).

- \( e_i \) es el **residual ordinario**: \( e_i = y_i - \hat{y}_i \).

- \( W_i \) representa el **peso o importancia de cada observación** en el modelo.

En modelos de **regresión lineal ordinaria**, se asume que **todas las observaciones tienen el mismo peso**, es decir, \( W_i = 1 \). Sin embargo, en modelos de regresión ponderada, los pesos pueden variar para dar mayor o menor importancia a ciertas observaciones.

El residual de Pearson se usa comúnmente en **modelos de regresión generalizada** (GLM) para evaluar el ajuste del modelo cuando las varianzas de los errores no son constantes.





</br></br>
<h3>Residual estandarizado</h3>

El **residual estandarizado** es una versión ajustada del **residual ordinario**, que toma en cuenta la **variabilidad de los errores** y la **influencia de cada observación** en el modelo. Se define como:

\[
d_i = \frac{e_i \sqrt{W_i}}{\sqrt{\hat{\sigma}^2 (1 - h_{ii})}}
\]

donde:

- \( d_i \) es el **residual estandarizado** para la observación \( i \).

- \( e_i \) es el **residual ordinario**: \( e_i = y_i - \hat{y}_i \).

- \( W_i \) es el **peso de la observación \( i \)**. En regresión lineal ordinaria, \( W_i = 1 \).

- \( \hat{\sigma}^2 \) es la **varianza estimada del error** en el modelo.

- \( h_{ii} \) es el **leverage**, es decir, la influencia de la observación \( i \) en su propia predicción.

Este residual ajusta la escala de los errores en función de la **influencia de cada observación en el modelo**. Es útil para identificar **valores atípicos**, ya que valores absolutos grandes de \( d_i \) sugieren que una observación se aleja significativamente del patrón general del modelo.



</br></br>
<h3>Residual studentizado</h3>

El **residual studentizado** es una versión ajustada del **residual estandarizado**, en la que se estima la varianza del error **sin incluir la observación \( i \)**. Esto permite una evaluación más robusta de la influencia de cada observación y facilita la detección de **valores atípicos**.

\[
r_i = \frac{e_i \sqrt{W_i}}{\sqrt{\hat{\sigma}^2_{(i)} (1 - h_{ii})}}
\]

donde:

- \( r_i \) es el **residual studentizado** para la observación \( i \).

- \( e_i \) es el **residual ordinario**: \( e_i = y_i - \hat{y}_i \).

- \( W_i \) es el **peso de la observación \( i \)**. En regresión lineal ordinaria, \( W_i = 1 \).

- \( \hat{\sigma}^2_{(i)} \) es la **varianza del error estimada sin incluir la observación \( i \)**.

- \( h_{ii} \) es el **leverage**, que mide la influencia de la observación \( i \) en su propia predicción.

El **residual studentizado** se usa principalmente para **detectar valores atípicos de manera más precisa**, ya que el cálculo de la varianza excluyendo la observación \( i \) reduce el impacto de valores extremos en la estimación de \( \sigma^2 \). 


En modelos bien ajustados con errores normales, se espera que la mayoría de los valores de \( r_i \) estén dentro del rango 
\([-2,2] \). Valores absolutos mayores a **3** sugieren que la observación \( i \) podría ser **un valor atípico significativo**.
A diferencia del residual estandarizado, este residual es más confiable para detectar valores extremos, ya que ajusta la varianza sin depender de la observación analizada.

Este tipo de residual es una herramienta clave en **diagnósticos de regresión**, especialmente cuando se busca identificar puntos que podrían afectar sustancialmente la estimación de los coeficientes del modelo.






</br></br>
<h3>Residual parcial</h3>

El **residual parcial** se obtiene eliminando una de las columnas de la matriz de diseño \( X \) y recalculando los residuales en el modelo modificado. Su principal utilidad radica en la **regresión lineal múltiple**, donde permite evaluar el impacto individual de una variable predictora en la variable respuesta.

Este residual se expresa matemáticamente como:

\[
e_{p,i} = Y_i - \hat{Y}_i^{(-j)}
\]

donde:

- \( e_{p,i} \) es el **residual parcial** de la observación \( i \).

- \( Y_i \) es el **valor observado** de la variable respuesta.

- \( \hat{Y}_i^{(-j)} \) es el **valor ajustado** para \( Y_i \) en un modelo donde la variable predictora \( X_j \) ha sido eliminada.


En **regresión múltiple**, cada predictor \( X_j \) puede estar asociado con otras variables del modelo. El **residual parcial** permite aislar el efecto de \( X_j \) sobre \( Y \) y evaluar si su inclusión en el modelo mejora la predicción.

Este tipo de residual es especialmente útil para:

- **Evaluar la importancia de una variable predictora** en el modelo.

- **Detectar colinealidad**, observando cómo cambia el ajuste al eliminar una variable.

- **Analizar la relación marginal entre \( X_j \) y \( Y \)**, ignorando el efecto de otras variables.


Si los **residuales parciales son grandes**, significa que la variable eliminada tenía un impacto significativo en la predicción. Si los residuales no muestran un patrón claro, la variable eliminada puede no estar contribuyendo significativamente al modelo.

Los **gráficos de residuales parciales** permiten visualizar la relación entre una variable predictora específica y la respuesta, lo que facilita la toma de decisiones sobre la inclusión o exclusión de predictores en el modelo.




</br></br>
<h3>Residual PRESS (Prediction Residual Sum of Squares)</h3>

El **residual PRESS** mide la diferencia entre el valor observado \( y_i \) y el valor predicho \( \hat{y}_{(i)} \), donde **\( \hat{y}_{(i)} \) es la estimación de \( y_i \) sin usar la observación \( i \) en el ajuste del modelo**. Este residual es clave en la **validación cruzada leave-one-out (LOO)**, ya que evalúa cómo se comportaría el modelo si la observación \( i \) no hubiera sido incluida en su entrenamiento.

\[
e_{(i)} = y_i - \hat{y}_{(i)} = \frac{e_i}{1 - h_{ii}}
\]

donde:

- \( e_{(i)} \) es el **residual PRESS** para la observación \( i \).

- \( e_i \) es el **residual ordinario**: \( e_i = y_i - \hat{y}_i \).

- \( h_{ii} \) es el **leverage**, que mide la influencia de la observación \( i \) en su propia predicción.

</br>
Este residual se usa para calcular la **suma de los cuadrados de los errores de predicción (PRESS)**:

  \[
  PRESS = \sum_{i=1}^{n} e_{(i)}^2
  \]

Un menor valor de **PRESS** indica un modelo con mejor capacidad de **predicción** en datos nuevos. Es útil para **detectar observaciones influyentes**, ya que valores grandes de \( e_{(i)} \) pueden indicar que una observación tiene un impacto significativo en el ajuste del modelo.

Si \( e_{(i)} \) es grande en valor absoluto, significa que la observación \( i \) **es difícil de predecir** sin incluirla en el modelo, lo que sugiere que podría ser un **valor atípico o influyente**. Valores pequeños de \( e_{(i)} \) sugieren que la observación \( i \) **no afecta significativamente la predicción** y que el modelo es **estable** ante su eliminación.


---

</br></br>
<h2>Funciones en **R** para obtener residuales</h2>

Para obtener los distintos tipos de residuales en **R**, se pueden utilizar las siguientes funciones:

<pre>
# Residuales generales
residuals(object, type = c("working", "response", "deviance", "pearson", "partial"))

# Residuales estandarizados
rstandard(object)

# Residuales studentizados
rstudent(object)
</pre>



</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>
El objetivo es ajustar un **modelo de regresión lineal ponderada**, donde la **media de \( Y \)** se modela en función de \( X \), utilizando como pesos los valores de \( W \). Una vez ajustado el modelo, se procederá a calcular los **valores ajustados** y los **residuales manualmente**, comparándolos posteriormente con los obtenidos mediante las funciones de **R**.


En este análisis, se dispone de informaciónsobre la variable respuesta (\( Y \)), la variable predictora (\( X \)) y los pesos asociados (\( W \)). 

Los datos correspondientes están disponibles en el siguiente enlace:

[Descargar datos de \( Y \), \( X \) y \( W \)](https://raw.githubusercontent.com/smramirezb/datos_ejemplos/refs/heads/main/dat_res.txt)



Para realizar este ajuste en **R**, se emplea la función `lm()` especificando el argumento `weights = w`, lo que permite asignar diferentes pesos a cada observación en el modelo de regresión.

Los **coeficientes estimados** mediante el ajuste del modelo definen la ecuación de la **regresión lineal ponderada**:

\[
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]


**Cálculo manual de los valores ajustados y residuales:**

Dado que la regresión es **ponderada**, los valores ajustados y los **residuales ordinarios** se calculan de la siguiente manera:

\[
e_i = y_i - \hat{y}_i
\]

\[
p_i = e_i \sqrt{w_i} \quad \text{(Residual de Pearson)}
\]

\[
r_i = \frac{e_i \sqrt{w_i}}{\sqrt{\hat{\sigma}^2 (1 - h_{ii})}} \quad \text{(Residual Estandarizado)}
\]

donde:

- \( e_i \) es el **residual ordinario**, que representa la diferencia entre el valor observado \( y_i \) y el valor ajustado \( \hat{y}_i \).

- \( p_i \) es el **residual de Pearson**, que ajusta la magnitud del error en función del peso \( w_i \), permitiendo evaluar la adecuación del modelo cuando las observaciones tienen varianzas heterogéneas.

- \( r_i \) es el **residual estandarizado**, el cual ajusta el error considerando tanto la varianza estimada \( \hat{\sigma}^2 \) como el leverage \( h_{ii} \), proporcionando una métrica útil para la detección de valores atípicos.



<pre>
# Lectura de datos
file <- "https://raw.githubusercontent.com/smramirezb/datos_ejemplos/refs/heads/main/dat_res.txt"
datos <- read.table(file = file, header = TRUE)

x<-datos$x
y<-datos$y
w<-datos$w


#  Ajuste del Modelo de Regresión Lineal Ponderada 
# Se ajusta el modelo de regresión ponderada usando los valores de 'w' como pesos
modelo_ponderado <- lm(y ~ x, weights = w)  

# Se pueden visualizar los resultados del modelo con:
summary(modelo_ponderado)

# Extraer Coeficientes del Modelo 
# Se obtienen los coeficientes estimados del modelo ajustado
beta_0 <- coef(modelo_ponderado)[1]  # Intercepto estimado
beta_1 <- coef(modelo_ponderado)[2]  # Pendiente estimada

# Cálculo de los Valores Ajustados Manualmente 
# Se calculan los valores ajustados manualmente utilizando la ecuación del modelo
y_hat_manual <- beta_0 + beta_1 * x  

# Cálculo de los Residuales ---
# Los residuales ordinarios representan la diferencia entre los valores observados y los valores ajustados
residuales_ordinarios <- y - y_hat_manual  

# Cálculo de los Residuales de Pearson
# Los residuales de Pearson ajustan el error ordinario usando la raíz cuadrada del peso
residuales_pearson_manual <- residuales_ordinarios * sqrt(w) 

# Crear una Tabla con los Resultados 
# Se crea un data frame con las variables, predicciones y residuales
resultados <- data.frame(
  X = x,
  Y = y,
  Pesos = w,
  Predicciones = y_hat_manual,
  Residuales_Ordinarios = residuales_ordinarios,
  Residuales_Pearson = residuales_pearson_manual
)

# Mostrar la tabla con los resultados
print(resultados)
</pre>



```{r,eval=FALSE,warning=FALSE,message=FALSE}
# Lectura de datos
file <- "https://raw.githubusercontent.com/smramirezb/datos_ejemplos/refs/heads/main/dat_res.txt"
datos <- read.table(file = file, header = TRUE)

x<-datos$x
y<-datos$y
w<-datos$w


#  Ajuste del Modelo de Regresión Lineal Ponderada 
# Se ajusta el modelo de regresión ponderada usando los valores de 'w' como pesos
modelo_ponderado <- lm(y ~ x, weights = w)  

# Se pueden visualizar los resultados del modelo con:
summary(modelo_ponderado)

# Extraer Coeficientes del Modelo 
# Se obtienen los coeficientes estimados del modelo ajustado
beta_0 <- coef(modelo_ponderado)[1]  # Intercepto estimado
beta_1 <- coef(modelo_ponderado)[2]  # Pendiente estimada

# Cálculo de los Valores Ajustados Manualmente 
# Se calculan los valores ajustados manualmente utilizando la ecuación del modelo
y_hat_manual <- beta_0 + beta_1 * x  

# Cálculo de los Residuales 
# Los residuales ordinarios representan la diferencia entre los valores observados y los valores ajustados
residuales_ordinarios <- y - y_hat_manual  

# Cálculo de los Residuales de Pearson 
# Los residuales de Pearson ajustan el error ordinario usando la raíz cuadrada del peso
residuales_pearson_manual <- residuales_ordinarios * sqrt(w) 

# Crear una Tabla con los Resultados 
# Se crea un data frame con las variables, predicciones y residuales
resultados <- data.frame(
  X = x,
  Y = y,
  Pesos = w,
  Predicciones = y_hat_manual,
  Residuales_Ordinarios = residuales_ordinarios,
  Residuales_Pearson = residuales_pearson_manual
)

# Mostrar la tabla con los resultados
print(resultados)
```


Los resultados de los **residuales ordinarios** y de **Pearson** son los siguientes:

<pre>
   X Y Pesos Predicciones Residuales_Ordinarios Residuales_Pearson
1 4 1   0.1     3.348739            -2.3487395        -0.74273664
2 6 2   0.1     3.710084            -1.7100840        -0.54077605
3 8 3   0.2     4.071429            -1.0714286        -0.47915742
4 7 4   0.1     3.890756             0.1092437         0.03454589
5 8 5   0.2     4.071429             0.9285714         0.41526977
6 5 4   0.9     3.529412             0.4705882         0.44643920
</pre>


Para comprobar que los cálculos manuales de los valores ajustados y los residuales coinciden con los obtenidos por las funciones de **R**, se realiza la siguiente comparación:


<pre>
# Lectura de datos
file <- "https://raw.githubusercontent.com/smramirezb/datos_ejemplos/refs/heads/main/dat_res.txt"
datos <- read.table(file = file, header = TRUE)

x<-datos$x
y<-datos$y
w<-datos$w

# Ajuste del Modelo de Regresión Lineal Ponderada ---
# Se ajusta un modelo de regresión lineal ponderada usando 'w' como pesos
modelo_ponderado <- lm(y ~ x, weights = w)  

# Obtención de Valores Ajustados y Residuales ---
# Valores ajustados (predicciones) obtenidos a partir del modelo ajustado por R
y_hat_r <- fitted(modelo_ponderado)

# Residuales ordinarios: Diferencia entre valores observados y valores ajustados
residuales_r <- residuals(modelo_ponderado)

# Residuales de Pearson: Residuales ordinarios escalados por la varianza estimada
residuales_pearson_r <- residuals(modelo_ponderado, type = "pearson")

# Comparación de Resultados Manuales y de R ---
# Se verifica si los cálculos manuales coinciden con los generados por R

comparacion_resultados <- data.frame(
  X = x,
  Y = y,
  Pesos = w,
  Predicciones_Manual = y_hat_manual,  # Predicciones calculadas manualmente
  Predicciones_R = y_hat_r,  # Predicciones generadas por R
  Residuales_Ordinarios_Manual = residuales_ordinarios,  # Residuales ordinarios manuales
  Residuales_R = residuales_r,  # Residuales ordinarios generados por R
  Residuales_Pearson_Manual = residuales_pearson_manual,  # Residuales de Pearson manuales
  Residuales_Pearson_R = residuales_pearson_r  # Residuales de Pearson generados por R
)

# Mostrar la tabla con la comparación de resultados
print(comparacion_resultados)
</pre>


```{r,eval=FALSE,warning=FALSE,message=FALSE}
# Lectura de datos
file <- "https://raw.githubusercontent.com/smramirezb/datos_ejemplos/refs/heads/main/dat_res.txt"
datos <- read.table(file = file, header = TRUE)

x<-datos$x
y<-datos$y
w<-datos$w

# Ajuste del Modelo de Regresión Lineal Ponderada ---
# Se ajusta un modelo de regresión lineal ponderada usando 'w' como pesos
modelo_ponderado <- lm(y ~ x, weights = w)  

# Obtención de Valores Ajustados y Residuales ---
# Valores ajustados (predicciones) obtenidos a partir del modelo ajustado por R
y_hat_r <- fitted(modelo_ponderado)

# Residuales ordinarios: Diferencia entre valores observados y valores ajustados
residuales_r <- residuals(modelo_ponderado)

# Residuales de Pearson: Residuales ordinarios escalados por la varianza estimada
residuales_pearson_r <- residuals(modelo_ponderado, type = "pearson")

# Comparación de Resultados Manuales y de R ---
# Se verifica si los cálculos manuales coinciden con los generados por R

comparacion_resultados <- data.frame(
  X = x,
  Y = y,
  Pesos = w,
  Predicciones_Manual = y_hat_manual,  # Predicciones calculadas manualmente
  Predicciones_R = y_hat_r,  # Predicciones generadas por R
  Residuales_Ordinarios_Manual = residuales_ordinarios,  # Residuales ordinarios manuales
  Residuales_R = residuales_r,  # Residuales ordinarios generados por R
  Residuales_Pearson_Manual = residuales_pearson_manual,  # Residuales de Pearson manuales
  Residuales_Pearson_R = residuales_pearson_r  # Residuales de Pearson generados por R
)

# Mostrar la tabla con la comparación de resultados
print(comparacion_resultados)
```

Los resultados de la implementación manual y con **R** es la siguiente:

<pre>
  X Y Pesos Predicciones_Manual Predicciones_R Residuales_Ordinarios_Manual Residuales_R Residuales_Pearson_Manual Residuales_Pearson_R
1 4 1   0.1            3.348739       3.348739                   -2.3487395   -2.3487395               -0.74273664          -0.74273664
2 6 2   0.1            3.710084       3.710084                   -1.7100840   -1.7100840               -0.54077605          -0.54077605
3 8 3   0.2            4.071429       4.071429                   -1.0714286   -1.0714286               -0.47915742          -0.47915742
4 7 4   0.1            3.890756       3.890756                    0.1092437    0.1092437                0.03454589           0.03454589
5 8 5   0.2            4.071429       4.071429                    0.9285714    0.9285714                0.41526977           0.41526977
6 5 4   0.9            3.529412       3.529412                    0.4705882    0.4705882                0.44643920           0.44643920
</pre>

---

**Regresión ponderada:**

En una **regresión ponderada**, los **pesos** asignados a cada observación determinan la influencia que tiene cada punto en la estimación de los coeficientes del modelo. Estos pesos permiten ajustar la importancia de cada observación, lo que es útil cuando los datos presentan heterocedasticidad (variabilidad no constante) o cuando algunas mediciones son más confiables que otras.

Los pesos \( w_i \) pueden interpretarse de diferentes maneras según el contexto del problema:

- **Como la inversa de la varianza de los errores**:
  
  \[
  w_i = \frac{1}{\sigma_i^2}
  \]
  
  Donde \( \sigma_i^2 \) representa la varianza del error para la observación \( i \). En este caso, se otorgan **mayores pesos** a las observaciones con menor variabilidad y **menores pesos** a las observaciones con alta dispersión.

- **Como una función de la confiabilidad de la observación**:

  - Las observaciones con **alta precisión** reciben **mayor peso**, influyendo más en la estimación del modelo.
  
  - Las observaciones con **mayor variabilidad o ruido** tienen **menor peso**, reduciendo su impacto en el ajuste.


</p>
</div>




<div class="caja-actividad">
<h3>Nota:</h3>
>
<p>
**Caso especial**: Si todos los pesos son iguales **(\( w_i = 1 \)** para todas las observaciones), la regresión ponderada **equivale a la regresión lineal ordinaria**.
</p>
>
</div>

<div class="caja-actividad">
<h3>Actividad:</h3>
>
<p>
- Repite el ejemplo anterior considerando una regresión lineal simple ordinaria. Compara los distintos tipos de residuales. 
</p>
>
</div>
