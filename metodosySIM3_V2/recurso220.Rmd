---
title: <span style="color:#686868"> **Estimación por máxima verosimilitud**</span>
author: "Métodos y Simulación Estadística"
output:
  html_document:
    code_folding: hide
    css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)

c1="#FF7F00"
c2="#=EB0C6"
c3="#034A94"
c4="#686868"

library(ggplot2)
library(paqueteMETODOS)
data(biomasa)
modelo=lm(log(bio_total) ~ diametro, data=biomasa)
```


</br></br>
<h3>Introducción</h3>

La **estimación por máxima verosimilitud (EMV)** es un enfoque probabilístico que busca determinar los valores de los parámetros desconocidos \( \beta_0, \beta_1 \) y \( \sigma^2 \) que **maximizan la verosimilitud de haber observado los datos de la muestra**. 


En el modelo de **regresión lineal simple**, se asume que la variable respuesta \( Y_i \) sigue una **distribución normal condicional** a la variable explicativa \( X_i \), de la siguiente forma:

$$
Y_i \mid X_i \sim N(\mu_i, \sigma^2), \\ \text{donde} \quad \mu_i =E[Y_i \mid X_i] = \beta_0 + \beta_1 X_i, \quad \sigma^2 = \text{constante}.
$$

 **¿Qué es variable y qué es fijo en este contexto?**
 
- **\( Y_i \) es una variable aleatoria**, ya que su valor cambia en cada realización del experimento.

- **\( X_i \) es fijo**, ya que se considera dado por el diseño del estudio o la muestra observada.

- **Los parámetros \( \beta_0, \beta_1, \sigma^2 \) son desconocidos** y deben ser estimados.

- **Los errores \( \varepsilon_i = Y_i - (\beta_0 + \beta_1 X_i) \) son variables aleatorias** con distribución normal \( N(0, \sigma^2) \).


Este modelo implica que, para cada valor fijo de \( X_i \), la variable respuesta \( Y_i \) sigue una distribución normal con **media condicional** \( \mu_i = \beta_0 + \beta_1 X_i \) y **varianza constante** \( \sigma^2 \), independientemente del valor de \( X_i \).


---

 **¿Cómo se determinan los valores de los parámetros que hacen más probable la ocurrencia de la muestra?**

Dado un conjunto de **\( n \) observaciones \( (X_i, Y_i) \)**, y asumiendo independencia condicional entre las \( Y_i \), la **función de verosimilitud conjunta** se construye como el producto de las densidades normales de cada \( Y_i \), dado \( X_i \):

$$
L(\beta_0, \beta_1, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{ -\frac{(Y_i - (\beta_0 + \beta_1 X_i))^2}{2\sigma^2} \right\}.
$$

Donde:

- **\( \mu_i = \beta_0 + \beta_1 X_i \)** representa la media condicional de \( Y_i \).

- **\( \sigma^2 \)** es constante y representa la varianza de los errores.


---

El método consiste en encontrar los valores de \( \beta_0, \beta_1 \) y \( \sigma^2 \) que **maximizan función de verosimilitud conjunta** \( L(\beta_0, \beta_1, \sigma^2) \), es decir, los que hacen más probable haber observado los datos de la muestra.


Bajo los supuestos de normalidad de los errores, se puede demostrar que los estimadores de máxima verosimilitud de \( \beta_0 \) y \( \beta_1 \) son **idénticos** a los obtenidos mediante **mínimos cuadrados ordinarios (MCO)**. Sin embargo, el **estimador de la varianza \( \sigma^2 \) difiere** entre ambos métodos:

- **En MCO**, la varianza de los errores se estima como:

  $$
  \hat{\sigma}^2_{\text{MCO}} = \frac{1}{n-2} \sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)^2
  $$

  donde \( n-2 \) en el denominador garantiza que sea un **estimador insesgado**.

- **En EMV**, la varianza se estima con:

  $$
  \hat{\sigma}^2_{\text{EMV}} = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i)^2
  $$

  donde el denominador es \( n \), lo que hace que sea un **estimador sesgado**, pero con menor varianza y, por lo tanto, más eficiente en términos de error cuadrático medio.

Resumiendo: 

- En la **regresión lineal bajo el enfoque de máxima verosimilitud**, **\( X_i \) se considera fijo y \( Y_i \) es una variable aleatoria**.

- **Los estimadores EMV de \( \beta_0 \) y \( \beta_1 \), bajo normalidad,  coinciden con los obtenidos por mínimos cuadrados ordinarios (MCO)**.

- **El estimador de la varianza \( \sigma^2 \) es diferente entre ambos métodos**: MCO usa \( n-2 \)  y resulta un estimador insesgado, mientras que EMV usa \( n \), lo que introduce un pequeño sesgo pero mejora la eficiencia del estimador.



</br></br>
<h3>Formulación</h3>

Una de las formas de representar el modelo de **regresión lineal simple** es mediante una formulación probabilística, que establece que la variable respuesta \( Y_i \), condicionada a un valor fijo de \( X_i \), sigue una **distribución normal**:

$$
Y_i \mid X_i \sim N(\mu_i, \sigma^2), \\ \text{donde} \quad \mu_i =E[Y_i \mid X_i] = \beta_0 + \beta_1 X_i, \quad \sigma^2 = \text{constante}.
$$

En la práctica, una muestra de datos observados consiste en un conjunto de **pares de valores**:

$$
(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n).
$$

Donde:

- \( x_i \) es un **valor fijo** en la muestra.

- \( y_i \) es una **realización observada** de la variable aleatoria \( Y_i \) para un valor concreto \( x_i \).

Estos valores observados se utilizan para **inferir los parámetros desconocidos** \( \beta_0, \beta_1, \sigma^2 \) mediante métodos como **máxima verosimilitud (EMV)** o **mínimos cuadrados ordinarios (MCO)**.


</br></br>
<h4>Función de verosimilitud</h4>

En el contexto de la **estimación por máxima verosimilitud (EMV)**, el objetivo es determinar los valores de los parámetros desconocidos \( \beta_0, \beta_1 \) y \( \sigma^2 \) que **maximizan la verosimilitud de los datos observados**, dada la estructura del modelo probabilístico.

Dado que el modelo de regresión lineal simple se expresa como:

$$
Y_i \mid X_i \sim N(\beta_0 + \beta_1 X_i, \sigma^2), \quad \text{para } i = 1,2, \dots, n,
$$

la densidad condicional de cada **variable aleatoria** \( Y_i \) dado \( X_i \) está dada por:

$$
f(Y_i \mid X_i; \beta_0, \beta_1, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} 
\exp \left\{ -\frac{(Y_i - \beta_0 - \beta_1 X_i)^2}{2\sigma^2} \right\}.
$$

En el modelo de regresión lineal, se asume que los **errores \( \varepsilon_i \) son independientes e identicamente distribuidos**, es decir:

$$
\varepsilon_i \sim \text{i.i.d. } N(0, \sigma^2).
$$

Esto implica que las **variables aleatorias \( Y_i \) son condicionalmente independientes** dado el conjunto de valores fijos \( X_1, \dots, X_n \), aunque no necesariamente sean independientes marginalmente. La independencia condicional permite escribir la **función de verosimilitud** como el producto de las densidades individuales:


$$
L(\beta_0, \beta_1, \sigma^2 | x_1, \dots, x_n, y_1, \dots, y_n) =
\prod_{i=1}^{n} f(y_i \mid x_i; \beta_0, \beta_1, \sigma^2).
$$

Sustituyendo la densidad normal de cada \( Y_i \):

$$
L(\beta_0, \beta_1, \sigma^2| x_1, \dots, x_n, y_1, \dots, y_n) = 
\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} 
\exp \left\{ -\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2} \right\}.
$$

Reescribiendo en una forma más compacta:

$$
L(\beta_0, \beta_1, \sigma^2| x_1, \dots, x_n, y_1, \dots, y_n) = \frac{1}{(2\pi\sigma^2)^{n/2}} 
\exp \left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2 \right\}.
$$

Respecto a esta función:

- La función de verosimilitud **no representa una probabilidad**, sino una **densidad conjunta de los datos observados** bajo los parámetros del modelo.

- **Las \( y_i \) y \( x_i \) representan valores observados** en la muestra, mientras que \( Y_i \) y \( X_i \) son variables aleatorias.

- La independencia asumida en la construcción de la verosimilitud **se refiere a la independencia condicional de los \( Y_i \) dado \( X_i \)**, no a su independencia marginal.



</br></br>
<h4>Función de Log-Verosimilitud</h4>

Dado que la función de verosimilitud es un **producto de términos exponenciales**, es conveniente trabajar con su **logaritmo natural**, lo que transforma productos en sumas y facilita la maximización:

$$
\ln L(\beta_0, \beta_1, \sigma^2) = l(\beta_0, \beta_1, \sigma^2).
$$

Expandiendo la expresión:

$$
l(\beta_0, \beta_1, \sigma^2) = -\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i)^2.
$$


El uso del **logaritmo de la verosimilitud** simplifica la maximización de la función de verosimilitud, permitiendo obtener los **estimadores de máxima verosimilitud (EMV)**.


</br></br>
<h4>Estimaciones máximo verosimil (EMV)</h4>

Para obtener los **estimadores de máxima verosimilitud (EMV)** de los parámetros \( \beta_0, \beta_1 \) y \( \sigma^2 \), se resuelven las ecuaciones normales obtenidas al derivar la log-verosimilitud con respecto a cada parámetro e igualar a cero. **Los datos observados \( (x_i, y_i) \) se utilizan para inferir los parámetros desconocidos del modelo**.

1. **Estimadores de los coeficientes \( \beta_0 \) y \( \beta_1 \)**  
   Derivando respecto a \( \beta_0 \) y \( \beta_1 \), se obtiene que los EMV coinciden con los estimadores de **mínimos cuadrados ordinarios (MCO)**:

   $$
   \hat{\beta}_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}, \quad
   \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
   $$

2. **Estimador de la varianza \( \sigma^2 \)**  
   Derivando respecto a \( \sigma^2 \), se obtiene el EMV de la varianza:

   $$
   \hat{\sigma}^2_{\text{EMV}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2.
   $$

   
De lo anterior se concluye que bajo el supuesto de normalidad, **los EMV de \( \beta_0 \) y \( \beta_1 \) coinciden con los estimadores obtenidos por MCO**.
**El estimador EMV de \( \sigma^2 \) es ligeramente sesgado**, pues usa \( n \) en lugar de \( n-2 \), pero es más eficiente en términos de varianza.


<div class="caja-nota">
> "Bajo el supuesto de normalidad, los EMV de \( \beta_0 \) y \( \beta_1 \) coinciden con los estimadores obtenidos por MCO."
</div>


<div class="caja-nota">
> "El estimador EMV de \( \sigma^2 \) es ligeramente sesgado, pues usa \( n \) en lugar de \( n-2 \), pero es más eficiente en términos de varianza."
</div>

---

La elección entre **MCO** y **EMV** depende del contexto y de si se busca una estimación insesgada o una basada en principios probabilísticos.




